{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40dcbfc6-b818-4d15-90ed-39cc12c2e8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccf8b739-fbc1-4be1-9b60-092843971af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/miniconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/macbook/miniconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet101_coco-586e9e4e.pth\" to /Users/macbook/.cache/torch/hub/checkpoints/deeplabv3_resnet101_coco-586e9e4e.pth\n",
      "100%|████████████████████████████████████████| 233M/233M [00:48<00:00, 5.06MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = models.segmentation.deeplabv3_resnet101(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5364f71-fd98-4773-8e6d-78fc2229bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"example.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95336ac1-c620-47ab-84c0-5d3c7ee98388",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e1f24cf-67e6-46cb-b4ca-e27d32faaf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = preprocess(image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59737257-8810-4b99-b215-bf3108479c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "input_batch = input_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bfccb271-5026-4a34-96e6-365ee89f3b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "\n",
    "    output = model(input_batch)['out'][0]\n",
    "output_predictions = output.argmax(0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81de9e4d-251a-46e6-9add-a98debbeb14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mask with background\n",
    "background_mask = np.where(output_predictions == 0, 0, 1)\n",
    "\n",
    "# mask original image\n",
    "foreground = np.array(image) * np.expand_dims(background_mask, axis=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1d472a6-b6a1-4d60-9606-d5bc6c553b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_image = Image.open('fon.jpeg')\n",
    "background_image = background_image.resize(image.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c16bc572-83ed-4d78-812b-eb021deefedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_array = np.array(background_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e6c4417-9b25-4dd1-be9c-597ef6c66ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "background = background_array * np.expand_dims(1 - background_mask, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fda64db2-d76e-487e-a10d-70960448ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "composite_image = foreground + background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b80a315-b967-40e7-a7a6-a8f1b52651ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_image = Image.fromarray(composite_image.astype('uint8'))\n",
    "result_image.show()\n",
    "result_image.save('result.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
